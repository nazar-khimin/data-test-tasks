# Databricks Notebook: Data Profiling

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, mean, stddev, count, percentile_approx

# Initialize Spark Session
spark = SparkSession.builder.appName("Data Profiling").getOrCreate()

# Load Dataset
df = spark.read.parquet("/dbfs/datasets/etl_input.parquet")

# Summary Statistics
df.describe().show()

# IQR Outlier Detection
q1 = df.approxQuantile("salary", [0.25], 0)[0]
q3 = df.approxQuantile("salary", [0.75], 0)[0]
iqr = q3 - q1

outliers = df.filter((col("salary") < q1 - 1.5 * iqr) | (col("salary") > q3 + 1.5 * iqr))
if outliers.count() > 0:
    print("Outliers detected!")
    outliers.show()

# Stop Spark Session
spark.stop()