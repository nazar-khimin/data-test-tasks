# Databricks Notebook: Data Quality Checks

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when

# Initialize Spark Session
spark = SparkSession.builder.appName("Data Quality Checks").getOrCreate()

# Load Dataset
df = spark.read.parquet("/dbfs/datasets/etl_input.parquet")

# Schema Validation
expected_schema = ["id", "name", "age", "department", "salary"]
assert set(df.columns) == set(expected_schema), "Schema mismatch detected!"

# Null Values Check
df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()

# Duplicate Check
duplicates = df.groupBy(df.columns).count().filter(col("count") > 1)
if duplicates.count() > 0:
    print("Duplicates found!")
    duplicates.show()

# Stop Spark Session
spark.stop()